HloModule jit_add, is_scheduled=true, entry_computation_layout={(f32[4,4099,5120]{2,0,1:T(4,128)})->(f32[4,4099,5120]{2,0,1:T(4,128)}, f32[1,1,5120]{2,1,0:T(1,128)})}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true,true}

region_0.3 {
  Arg_1.5 = f32[]{:T(128)} parameter(1), metadata={op_name="jit(add)/jit(main)/reduce_sum[axes=(0, 1)]"}
  Arg_0.4 = f32[]{:T(128)} parameter(0), metadata={op_name="jit(add)/jit(main)/reduce_sum[axes=(0, 1)]"}
  ROOT add.6 = f32[]{:T(128)} add(Arg_0.4, Arg_1.5), metadata={op_name="jit(add)/jit(main)/reduce_sum[axes=(0, 1)]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=689}
}

ENTRY main.10 {
  constant.2 = f32[]{:T(128)} constant(0)
  Arg_0.1 = f32[4,4099,5120]{2,0,1:T(4,128)} parameter(0)
  reduce.7 = f32[5120]{0:T(1024)} reduce(Arg_0.1, constant.2), dimensions={0,1}, to_apply=region_0.3, metadata={op_name="jit(add)/jit(main)/reduce_sum[axes=(0, 1)]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=689}
  copy.1 = f32[4,4099,5120]{2,0,1:T(4,128)} copy(Arg_0.1)
  bitcast.1 = f32[1,1,5120]{2,1,0:T(1,128)} bitcast(reduce.7)
  ROOT tuple.1 = (f32[4,4099,5120]{2,0,1:T(4,128)}, f32[1,1,5120]{2,1,0:T(1,128)}) tuple(copy.1, bitcast.1)
} // main.10

