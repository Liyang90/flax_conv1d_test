HloModule jit__unnamed_wrapped_function_, is_scheduled=true, entry_computation_layout={(f32[5120]{0:T(1024)}, f32[4,1,5120]{2,1,0:T(1,128)}, f32[4,4096,5120]{2,1,0:T(8,128)})->(f32[]{:T(128)}, f32[4,4096,5120]{2,1,0:T(8,128)}, f32[5120]{0:T(1024)}, f32[4,1,5120]{2,1,0:T(1,128)})}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true,true,true,true}

fused_computation {
  param_1.23 = bf16[4,8,513,5120]{3,2,1,0:T(8,128)(2,1)} parameter(1)
  constant.21 = f32[]{:T(128)} constant(-inf)
  pad.6 = bf16[4,8,516,5120]{3,2,1,0:T(8,128)(2,1)} pad(param_1.23, constant.21), padding=0_0x0_0x0_3x0_0, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  iota.3 = s32[4,8,516,5120]{3,2,1,0:T(8,128)} iota(), iota_dimension=2, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  constant.20 = s32[] constant(513), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  broadcast.12 = s32[4,8,516,5120]{3,2,1,0:T(8,128)} broadcast(constant.20), dimensions={}, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  compare.4 = pred[4,8,516,5120]{3,2,1,0:T(8,128)(4,1)} compare(iota.3, broadcast.12), direction=GE, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  param_0.24 = bf16[4,7,3,5120]{3,2,1,0:T(4,128)(2,1)} parameter(0)
  constant.23 = f32[]{:T(128)} constant(0)
  pad.5 = bf16[4,8,516,5120]{3,2,1,0:T(8,128)(2,1)} pad(param_0.24, constant.23), padding=0_0x0_1x513_0x0_0, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  broadcast.10 = f32[4,8,516,5120]{3,2,1,0:T(8,128)} broadcast(constant.21), dimensions={}, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  select.2 = bf16[4,8,516,5120]{3,2,1,0:T(8,128)(2,1)} select(compare.4, pad.5, broadcast.10), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  ROOT maximum.1 = bf16[4,8,516,5120]{3,2,1,0:T(8,128)(2,1)} maximum(pad.6, select.2), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
} // fused_computation

fused_computation.3.clone.clone.clone.clone {
  param_0.20 = bf16[4,8,516,5120]{3,2,1,0:T(8,128)(2,1)} parameter(0)
  copy.13 = bf16[4,8,516,5120]{3,1,0,2:T(8,128)(2,1)} copy(param_0.20), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  ROOT bitcast.6 = bf16[32,516,5120]{2,0,1:T(8,128)(2,1)} bitcast(copy.13), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
}

bitcast_fusion {
  bf16input = f32[4,1,5120]{2,1,0:T(1,128)} parameter(0)
  ROOT bitcast.7 = f32[4,1,5120]{2,1,0:T(1,128)} bitcast(bf16input)
}

fused_computation.6 {
  param_3.14 = pred[32,513]{0,1:T(8,128)(4,1)} parameter(3)
  broadcast.16 = pred[32,513,5120]{2,0,1:T(8,128)(4,1)} broadcast(param_3.14), dimensions={0,1}, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  param_1.19 = bf16[4,8,516,5120]{3,2,1,0:T(8,128)(2,1)} parameter(1)
  fusion.10 = bf16[32,516,5120]{2,0,1:T(8,128)(2,1)} fusion(param_1.19), kind=kLoop, calls=fused_computation.3.clone.clone.clone.clone, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  param_0.26 = f32[4,1,5120]{2,1,0:T(1,128)} parameter(0)
  fusion.16 = f32[4,1,5120]{2,1,0:T(1,128)} fusion(param_0.26), kind=kLoop, calls=bitcast_fusion
  convolution.5 = f32[32,513,5120]{2,0,1:T(8,128)} convolution(fusion.10, fusion.16), window={size=4}, dim_labels=b0f_0io->b0f, feature_group_count=5120, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  constant.22 = f32[]{:T(128)} constant(0)
  broadcast.15 = f32[32,513,5120]{2,0,1:T(8,128)} broadcast(constant.22), dimensions={}, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  select.5 = f32[32,513,5120]{2,0,1:T(8,128)} select(broadcast.16, convolution.5, broadcast.15), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  param_2.13 = f32[5120]{0:T(1024)} parameter(2)
  broadcast.14 = f32[32,513,5120]{2,0,1:T(8,128)} broadcast(param_2.13), dimensions={2}, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/add" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=689}
  ROOT add.14.clone.1 = f32[32,513,5120]{2,0,1:T(8,128)} add(select.5, broadcast.14), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/add" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=689}
} // fused_computation.6

fused_computation.8 {
  param_0.23 = f32[4,4096,5120]{2,1,0:T(8,128)} parameter(0)
  ROOT copy.15 = bf16[4,4096,5120]{2,0,1:T(4,128)(2,1)} copy(param_0.23), metadata={op_name="input"}
}

fused_computation.9 {
  param_0.28 = f32[4099]{0:T(1024)} parameter(0)
  ROOT broadcast.18 = bf16[4,4099,5120]{2,0,1:T(4,128)(2,1)} broadcast(param_0.28), dimensions={1}, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/pad[padding_config=((0, 0, 0), (0, 3, 0), (0, 0, 0))]" source_file="/tmp/ipykernel_2694218/2333499347.py" source_line=63}
}

fused_computation.7 {
  param_0.27 = f32[4,4096,5120]{2,1,0:T(8,128)} parameter(0)
  fusion.12 = bf16[4,4096,5120]{2,0,1:T(4,128)(2,1)} fusion(param_0.27), kind=kLoop, calls=fused_computation.8, metadata={op_name="input"}
  param_1.20 = f32[4099]{0:T(1024)} parameter(1)
  fusion.13 = bf16[4,4099,5120]{2,0,1:T(4,128)(2,1)} fusion(param_1.20), kind=kLoop, calls=fused_computation.9, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/pad[padding_config=((0, 0, 0), (0, 3, 0), (0, 0, 0))]" source_file="/tmp/ipykernel_2694218/2333499347.py" source_line=63}
  ROOT convolution.6 = f32[4,1,5120]{2,1,0:T(1,128)} convolution(fusion.12, fusion.13), window={size=4099 pad=3_3}, dim_labels=f0b_i0o->0bf, batch_group_count=5120, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(2, 0, 1), rhs_spec=(2, 0, 1), out_spec=(1, 2, 0)) feature_group_count=1 batch_group_count=5120 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
}

region_1.24 {
  Arg_0.25 = f32[]{:T(128)} parameter(0), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/conv1d/reduce_sum[axes=(0, 1)]"}
  Arg_1.26 = f32[]{:T(128)} parameter(1), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/conv1d/reduce_sum[axes=(0, 1)]"}
  ROOT add.27 = f32[]{:T(128)} add(Arg_0.25, Arg_1.26), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/conv1d/reduce_sum[axes=(0, 1)]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=689}
}

fused_computation.10 {
  param_0.31 = f32[4099]{0:T(1024)} parameter(0)
  broadcast.19 = f32[4,4099,5120]{2,0,1:T(4,128)} broadcast(param_0.31), dimensions={1}, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/pad[padding_config=((0, 0, 0), (0, 3, 0), (0, 0, 0))]" source_file="/tmp/ipykernel_2694218/2333499347.py" source_line=63}
  constant.24 = f32[]{:T(128)} constant(0)
  ROOT reduce.0 = f32[5120]{0:T(1024)} reduce(broadcast.19, constant.24), dimensions={0,1}, to_apply=region_1.24, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/conv1d/reduce_sum[axes=(0, 1)]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=689}
}

region_0.16 {
  Arg_0.17 = f32[]{:T(128)} parameter(0), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/reduce_sum[axes=(0, 1, 2)]"}
  Arg_1.18 = f32[]{:T(128)} parameter(1), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/reduce_sum[axes=(0, 1, 2)]"}
  ROOT add.19 = f32[]{:T(128)} add(Arg_0.17, Arg_1.18), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/reduce_sum[axes=(0, 1, 2)]" source_file="/tmp/ipykernel_2694218/456449586.py" source_line=10}
}

fused_computation.11 {
  param_0.34 = f32[4,4104,5120]{2,1,0:T(8,128)} parameter(0)
  slice.5 = f32[4,4096,5120]{2,1,0:T(8,128)} slice(param_0.34), slice={[0:4], [0:4096], [0:5120]}, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/slice[start_indices=(0, 0, 0) limit_indices=(4, 4096, 5120) strides=None]" source_file="/tmp/ipykernel_2694218/2333499347.py" source_line=63}
  constant.25 = f32[]{:T(128)} constant(0)
  reduce.1 = f32[]{:T(128)} reduce(slice.5, constant.25), dimensions={0,1,2}, to_apply=region_0.16, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/reduce_sum[axes=(0, 1, 2)]" source_file="/tmp/ipykernel_2694218/456449586.py" source_line=10}
  ROOT tuple = (f32[]{:T(128)}, f32[4,4096,5120]{2,1,0:T(8,128)}) tuple(reduce.1, slice.5)
}

ENTRY main.31 {
  constant.26 = bf16[]{:T(256)} constant(0)
  constant.13 = pred[32,513]{0,1:T(8,128)(4,1)} constant({...}), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  constant.3 = f32[]{:T(128)} constant(6.10351562e-05)
  constant.5 = f32[4099]{0:T(1024)} constant({...}), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/pad[padding_config=((0, 0, 0), (0, 3, 0), (0, 0, 0))]" source_file="/tmp/ipykernel_2694218/2333499347.py" source_line=63}
  Arg_2.3 = f32[4,4096,5120]{2,1,0:T(8,128)} parameter(2), metadata={op_name="input"}
  Arg_0.1 = f32[5120]{0:T(1024)} parameter(0), metadata={op_name="params[\'params\'][\'conv1d\'][\'bias\']"}
  Arg_1.2 = f32[4,1,5120]{2,1,0:T(1,128)} parameter(1), metadata={op_name="params[\'params\'][\'conv1d\'][\'kernel\']"}
  pad = bf16[4,4104,5120]{2,1,0:T(8,128)(2,1)} pad(Arg_2.3, constant.26), padding=0_0x3_5x0_0, metadata={op_name="input"}
  reshape.3 = bf16[4,8,513,5120]{3,2,1,0:T(8,128)(2,1)} reshape(pad), metadata={op_name="input"}
  slice.3 = bf16[4,7,3,5120]{3,2,1,0:T(4,128)(2,1)} slice(reshape.3), slice={[0:4], [1:8], [0:3], [0:5120]}, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  fusion = bf16[4,8,516,5120]{3,2,1,0:T(8,128)(2,1)} fusion(slice.3, reshape.3), kind=kLoop, calls=fused_computation, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  fusion.8 = f32[32,513,5120]{2,0,1:T(8,128)} fusion(Arg_1.2, fusion, Arg_0.1, constant.13), kind=kOutput, calls=fused_computation.6, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  copy.4 = f32[32,513,5120]{2,1,0:T(8,128)} copy(fusion.8), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/add" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=689}
  reshape.7 = f32[4,4104,5120]{2,1,0:T(8,128)} reshape(copy.4), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/add" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=689}
  fusion.15 = (f32[]{:T(128)}, f32[4,4096,5120]{2,1,0:T(8,128)}) fusion(reshape.7), kind=kLoop, calls=fused_computation.11, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/reduce_sum[axes=(0, 1, 2)]" source_file="/tmp/ipykernel_2694218/456449586.py" source_line=10}
  get-tuple-element = f32[]{:T(128)} get-tuple-element(fusion.15), index=0, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/reduce_sum[axes=(0, 1, 2)]" source_file="/tmp/ipykernel_2694218/456449586.py" source_line=10}
  multiply.22 = f32[]{:T(128)} multiply(get-tuple-element, constant.3), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/mul" source_file="/tmp/ipykernel_2694218/456449586.py" source_line=10}
  get-tuple-element.1 = f32[4,4096,5120]{2,1,0:T(8,128)} get-tuple-element(fusion.15), index=1, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/reduce_sum[axes=(0, 1, 2)]" source_file="/tmp/ipykernel_2694218/456449586.py" source_line=10}
  fusion.11 = f32[4,1,5120]{2,1,0:T(1,128)} fusion(Arg_2.3, constant.5), kind=kOutput, calls=fused_computation.7, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(2, 0, 1), rhs_spec=(2, 0, 1), out_spec=(1, 2, 0)) feature_group_count=1 batch_group_count=5120 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  fusion.14 = f32[5120]{0:T(1024)} fusion(constant.5), kind=kLoop, calls=fused_computation.10, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/conv1d/reduce_sum[axes=(0, 1)]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=689}
  ROOT tuple.30 = (f32[]{:T(128)}, f32[4,4096,5120]{2,1,0:T(8,128)}, f32[5120]{0:T(1024)}, f32[4,1,5120]{2,1,0:T(1,128)}) tuple(multiply.22, get-tuple-element.1, fusion.14, fusion.11)
} // main.31

