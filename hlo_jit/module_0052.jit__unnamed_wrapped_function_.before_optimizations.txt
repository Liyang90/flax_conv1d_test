HloModule jit__unnamed_wrapped_function_, entry_computation_layout={(f32[5120]{0:T(1024)}, f32[4,1,5120]{2,1,0:T(1,128)}, f32[4,4096,5120]{2,1,0:T(8,128)})->(f32[]{:T(128)}, f32[4,4096,5120]{2,1,0:T(8,128)}, f32[5120]{0:T(1024)}, f32[4,1,5120]{2,1,0:T(1,128)})}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true,true,true,true}

region_0.16 {
  Arg_0.17 = f32[] parameter(0), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/reduce_sum[axes=(0, 1, 2)]"}
  Arg_1.18 = f32[] parameter(1), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/reduce_sum[axes=(0, 1, 2)]"}
  ROOT add.19 = f32[] add(Arg_0.17, Arg_1.18), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/reduce_sum[axes=(0, 1, 2)]" source_file="/tmp/ipykernel_2694218/456449586.py" source_line=10}
}

region_1.24 {
  Arg_0.25 = f32[] parameter(0), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/conv1d/reduce_sum[axes=(0, 1)]"}
  Arg_1.26 = f32[] parameter(1), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/conv1d/reduce_sum[axes=(0, 1)]"}
  ROOT add.27 = f32[] add(Arg_0.25, Arg_1.26), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/conv1d/reduce_sum[axes=(0, 1)]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=689}
}

ENTRY main.31 {
  Arg_2.3 = f32[4,4096,5120]{2,1,0} parameter(2), metadata={op_name="input"}
  Arg_1.2 = f32[4,1,5120]{2,1,0} parameter(1), metadata={op_name="params[\'params\'][\'conv1d\'][\'kernel\']"}
  convolution.9 = f32[4,4099,5120]{2,1,0} convolution(Arg_2.3, Arg_1.2), window={size=4 pad=3_3}, dim_labels=b0f_0io->b0f, feature_group_count=5120, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=5120 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  Arg_0.1 = f32[5120]{0} parameter(0), metadata={op_name="params[\'params\'][\'conv1d\'][\'bias\']"}
  reshape.10 = f32[1,1,5120]{2,1,0} reshape(Arg_0.1), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/reshape[new_sizes=(1, 1, 5120) dimensions=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=688}
  broadcast.11 = f32[1,1,5120]{2,1,0} broadcast(reshape.10), dimensions={0,1,2}, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/add" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=689}
  reshape.12 = f32[5120]{0} reshape(broadcast.11), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/add" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=689}
  broadcast.13 = f32[4,4099,5120]{2,1,0} broadcast(reshape.12), dimensions={2}, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/add" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=689}
  add.14 = f32[4,4099,5120]{2,1,0} add(convolution.9, broadcast.13), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/conv1d/add" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=689}
  slice.15 = f32[4,4096,5120]{2,1,0} slice(add.14), slice={[0:4], [0:4096], [0:5120]}, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(BlockJAX_1)/slice[start_indices=(0, 0, 0) limit_indices=(4, 4096, 5120) strides=None]" source_file="/tmp/ipykernel_2694218/2333499347.py" source_line=63}
  constant.8 = f32[] constant(0)
  reduce.20 = f32[] reduce(slice.15, constant.8), dimensions={0,1,2}, to_apply=region_0.16, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/reduce_sum[axes=(0, 1, 2)]" source_file="/tmp/ipykernel_2694218/456449586.py" source_line=10}
  constant.7 = f32[] constant(83886080)
  divide.21 = f32[] divide(reduce.20, constant.7), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/div" source_file="/tmp/ipykernel_2694218/456449586.py" source_line=10}
  constant.6 = f32[] constant(5120)
  multiply.22 = f32[] multiply(divide.21, constant.6), metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/mul" source_file="/tmp/ipykernel_2694218/456449586.py" source_line=10}
  constant.4 = f32[] constant(6.10351562e-05)
  broadcast.5 = f32[4,4096,5120]{2,1,0} broadcast(constant.4), dimensions={}
  pad.23 = f32[4,4099,5120]{2,1,0} pad(broadcast.5, constant.8), padding=0_0x0_3x0_0, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/pad[padding_config=((0, 0, 0), (0, 3, 0), (0, 0, 0))]" source_file="/tmp/ipykernel_2694218/2333499347.py" source_line=63}
  reduce.28 = f32[5120]{0} reduce(pad.23, constant.8), dimensions={0,1}, to_apply=region_1.24, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/conv1d/reduce_sum[axes=(0, 1)]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=689}
  convolution.29 = f32[4,1,5120]{2,1,0} convolution(Arg_2.3, pad.23), window={size=4099 pad=3_3}, dim_labels=f0b_i0o->0bf, batch_group_count=5120, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/transpose(jvp(BlockJAX_1))/conv1d/conv_general_dilated[window_strides=(1,) padding=((3, 3),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(2, 0, 1), rhs_spec=(2, 0, 1), out_spec=(1, 2, 0)) feature_group_count=1 batch_group_count=5120 precision=None preferred_element_type=None]" source_file="/home/liyanglu/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=663}
  ROOT tuple.30 = (f32[], f32[4,4096,5120]{2,1,0}, f32[5120]{0}, f32[4,1,5120]{2,1,0}) tuple(multiply.22, slice.15, reduce.28, convolution.29)
} // main.31

